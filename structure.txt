Pipeline sharing in and across collaborative environments

Sharing a pipeline developed in a collaborative environment such as a Jupyter notebook or Google collab is closely
tied to the data that it ingests. In the case of local file accesses, the developed pipeline can be run/executed
only in the development environment.

This poses a challenge, when the pipeline or its results need to be shared for the purposes of a collaborative study
across different organizations. In cases where the collaboration is established as the beginning of a research study/or a project,
there are usually efforts to establish coherent workflows for pipeline, data and result sharing. For such puproses,
data scientists can use one of the many cloud implementations of Jupyter notebooks offered by the major cloud vendors.
Azure Notebooks, Amazon SageMaker, Google Collab, Kaggle Notebooks are just a subset of the current market offerings in
this space.

Such a setup requires effort from developers from the beginning of the project to establish the necessary data pipelines.
 However, many independently collected results from researchers and data scientists don't require starting an official
  project where all collaborators are working and have access to the same environments. Collaborations arise
  after initial analysis have been performed and the results are already stored. Additionally, in cases of paper reviews,
   result verification processes, validation of simulation results, the pipelines need to be verified by outside experts.

For example, hospitals collect data locally in database servers. The data stored is in tabular format,
on on-premise servers. When medical researchers need to perform an analysis, they take a subset of the data with
accordance to general data protection laws, store it locally on their machine and processthe data with statistical
tools, such as R or Python. In such cases, migrating the data and the pipelines to a paid cloud service at that stage
would pose an unnecessary engineering and financial overhead.

However, when the data and/or findings needs to be shared either for verification or further study, just transferring
the analysis scripts in Jupyter is not sufficient for the results to be reproduced or the pipeline updated.
 Other users need access to the data and adjust the access of the data to the pipeline.
 The data being shared and the pipeline rewritten should be an easy enough task to everyone who has access
 to the programming scripts and the data. We performed an initial scraping of GitHub on 100 pipelines that ingest
 data and process it in the shape of a dataframe and found that only X of the pipelines can be executed when the data
 is shared. The rest of the pipelines had invalid data access even with the shared data or did not provide the data on
 GitHub at all. This initial analysis shows that only X percent of the pipelines that we selected can be reproduced
 out of the box despite being public repositories.

 Such low numbers point to the fact that reproducibility of results and reusability of the pipelines are non-trivial
 tasks to perform, especially when outside collaborators need to work with previously written code. To bridge this gap,
 we propose a system prototype for efficient pipeline sharing and reproduction of results, using existing resources,
without the need to migrate the source files and invest in cloud resources.


We are focusing on pipelines that take data stored in local flat files, such as parquet, or comma-separated-values files.
Previous analysis of the GitHub data proccessing landscape has shown that analytics operations in Python are performed
on dataframes in X percent of the cases. Dataframes offer easy to use and intuitive semantics to process tabular and
structured data. In this paper, we utilize the dataframes structure, to move these data to a database system.
We note that the principles that we outline in this paper can be used with any object storage or file system.

Given a Python pipeline, written as a script or Jupyter Notebook, we perform static code analysis of the script, we
 extract the information where the data file is located and move it to a database backend. For the general purpose of
  this system, we assume that the database backend is an existing DBMS, monitored and managed by an authorized person.
  However, we also provide a standalone portable implementation, where the database system is containerized and can be
  shipped across different servers and development environments. Our prototype system takes the two inputs, the name of the pipeline file,
  as well as the address/port combination for the database backend.

  Credentials for a new user are generated, database inside this server is created for the particular project,
  the permissions for the user are limited to accessing and changing only this database. The data is moved from the local
  client storage to a new table in the database. The output of our prototype system is a new, rewritten pipeline with
  an embedded connection to the database and adjusted data access to include the data from the database instead of the
  local file on the original client. In such shape, the pipeline can be executed in different environments with consistent
  access to the original input data.

 On the same subset of Github pipelines, we found that after using our system prototype, we can execute X percent of the
 pipelines out of the box, compared to X percent without using the system. Additionally, we analyze the potential of using
 the database environment for additional operations to store the state of the executed pipelines.

