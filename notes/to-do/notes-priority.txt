Filter out the data processing pipelines that are using native python (done)
Filter out all repositories that are using data processing frameworks (done)
Show that import of pandas is actually processing data (false positives) (done)
    - Count how many repositories actually read data! (done)

How many pipelines are reading data? (native + DPF) (done)
What are the ways in which data is being read? (done)

Coverage of Stork on the full set of pipelines.
    - Yearly breakdown (done)
    - Percentage of repositories that read data (done)
    - Categorize data reading methods (done)

Run the experiments on the current state.
Separate the py_read pipelines from the lib_read pipelines from the results.
Number of pipelines that can be rewritten and run.

Execution Scheme:

    - Try to execute pipelines and report file not found.
    - These are pipelines relevant to be translated and rewritten?
    - Pipelines containing main method or calls to functions - filter these out

Methods of interest:
    - numpy: load, loadtxt, genfromtxt, fromfile

    - pandas: read_table, read_csv, read_pickle, read_excel, Excel_file, read_json, read_html, read_xml, read_hdf, HDFStore, read_feather
    read_parquet, read_orc, read_sas, read_spss, read_sql_table, read_sql, read_sql_query, read_gbq, read_stata

    - cudf: read_csv, read_text, read_json, read_parquet, read_orc, read_hdf, read_feather, read_avro

    - pyspark:
        - SQL: .csv, .format, .jdbc, .json, .load, .option, .options, .orc, .parquet, .schema, .table, .text
        - Structured Streaming: .csv, .format, .json, .load, .option, .options, .orc, .parquet, .schema, .table, .text
        - Pandas API: .read_table, .read_delta, .read_parquet, .read_orc, .read_spark_io, .read_csv, .read_clipboard, .read_excel, .read_json, .read_html, .read_sql_table, .read_sql_query, .read_sql

    - spark

    - dask: .read_text, .read_avro, .read_csv, .read_parquet, .read_hdf, .read_orc, .read_json, .read_sql_table, .read_sql_query, .read_sql, .read_table, .read_fwf

    - arrow: .OSFile, .memory_map, .BufferReader, .read, .open_file, .csv.open_csv, .csv.read_csv, .feather.read_feather, .feather.read_table, .json.ReadOptions, .json.read_json, .parquet.ParquetDataset, .parquet.ParquetFile, .parquet.read_table, .parquet.read_metadata, .parquet.read_pandas, .parquet.read_schema, .orc.ORCFile, .orc.ORCWriter, .orc.read_table, .orc.write_table

    - duckdb: .read_csv, .read_parquet, .sql, .json

    - modin: .read_table, .read_csv, .read_pickle, .read_excel, .Excel_file, .read_json, .read_html, .read_xml, .read_hdf, .HDFStore, .read_feather
    .read_parquet, .read_orc, .read_sas, .read_spss, .read_sql_table, .read_sql, .read_sql_query, .read_gbq, .read_stata

    - polars: .read_csv, .read_csv_batched, .scan_csv, .read_ipc, .read_ipc_stream, .scan_ipc, .read_parquet, .scan_parquet, .read_database, .read_database_uri, .read_json, .read_ndjson, .scan_ndjson, .read_avro, .read_excel, .scan_iceberg, .scan_delta, .read_delta, .scan_pyarrow_dataset

    - dplyr

    - clickhouse

    - datatable: .iread, .fread

    _____________________________________

    ML

    - scikit_learn: datasets
    - tensorflow: data, datasets
    - torch: data, datasets
    - keras: datasets, image_dataset_from_directory, timeseries_dataset_from_array, text_dataset_from_directory, audio_dataset_from_directory

    ______________________________________

    PYTHON

    - open()