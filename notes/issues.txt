Issues when working on a complete repository:
    * Datasets for each pipeline are added to the general list of datasets
    * Inputs and datasets are intertwinned and sometimes conflicted

    * Retrieving an input/dataset from a variable, creates new assignment for each new variable call.
    * Duplicating of inputs needs to be removed.

Reasons for duplicating the inputs:
    * Not having clean access to the data_file field.
    * Filtering of data_source works on ambiguous fields.

Places to look at:
    * retrieve_variable_from_assignment()
       * all methods that invoke it.

Resolving duplicates:
    * The data structures for assignments, inputs, and datasets are kept empty after one iteration
    * Check if deep copies resolve the issue, and how they reflect on performance